{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "cache_folder = \"../models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', cache_dir=cache_folder)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=cache_folder, num_labels=2)\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.0954, 0.2594]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7000/7000 [00:00<00:00, 146410.46 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 118182.70 examples/s]\n",
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 77351.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data_foder = \"../data\"\n",
    "ds = load_dataset(\"tarudesu/ViCTSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Comment', 'Constructiveness', 'Toxicity', 'Title', 'Topic'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'Comment', 'Constructiveness', 'Toxicity', 'Title', 'Topic'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Comment', 'Constructiveness', 'Toxicity', 'Title', 'Topic'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 167.59ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 189.07ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 166.99ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239221"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds\n",
    "# save to csv file for each split\n",
    "# Save to CSV files\n",
    "ds['train'].to_csv('../data/ViCTSD/train.csv', index=False)\n",
    "ds['validation'].to_csv('../data/ViCTSD/dev.csv', index=False)\n",
    "ds['test'].to_csv('../data/ViCTSD/test.csv', index=False)\n",
    "\n",
    "# load from csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ViCTSD/test.csv')\n",
    "df = df[[\"Comment\", \"Toxicity\"]]\n",
    "df = df.rename(columns={\"Comment\": \"text\", \"Toxicity\": \"label\"})\n",
    "df.to_csv('../data/ViCTSD/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ViSHD/dev.csv')\n",
    "df = df.rename(columns={\"free_text\": \"text\", \"label_id\": \"label\"})\n",
    "df.to_csv('../data/ViSHD/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic-chat-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
